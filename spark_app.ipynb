{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNER JOIN Result:\n",
      "('2021-08-16 02:14:01', '123456_20210810', '123456', 0.3, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:54:43', '123451_20210810', '123451', 0.15, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:04:29', '123452_20210810', '123452', 0.15, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:39:05', '123453_20210810', '123453', 0.15, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:14:19', '123454_20210810', '123454', 0.3, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:14:02', '123455_20210810', '123455', 0.15, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:09:14', '123457_20210810', '123457', 0.75, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:06:21', '123459_20210810', '123459', 0.15, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:57:22', '234591_20210810', '234591', 0.3, 'SMS', '2021-08-10')\n",
      "('2021-08-16 04:02:10', '234592_20210810', '234592', 0.3, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:00:51', '234593_20210810', '234593', 0.15, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:41:21', '234594_20210810', '234594', 0.3, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:06:27', '234595_20210810', '234595', 1.5, 'SMS', '2021-08-10')\n",
      "('2021-08-16 02:15:02', '234596_20210810', '234596', 0.15, 'SMS', '2021-08-10')\n",
      "('2021-08-16 11:46:51', '234597_20210810', '234597', 0.75, 'SMS', '2021-08-10')\n",
      "('2021-08-16 12:31:13', '234598_20210709', '234598', 0.15, 'SMS', '2021-07-09')\n",
      "('2021-08-16 13:13:21', '234599_20210709', '234599', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 02:31:58', '345991_20210709', '345991', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 08:29:18', '345992_20210709', '345992', 0.15, 'SMS', '2021-07-09')\n",
      "('2021-08-16 02:19:00', '345993_20210709', '345993', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 03:33:36', '345994_20210709', '345994', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 04:20:03', '345995_20210709', '345995', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 05:59:14', '345996_20210709', '345996', 0.75, 'SMS', '2021-07-09')\n",
      "('2021-08-16 10:12:01', '345997_20210709', '345997', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 06:57:57', '345998_20210709', '345998', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 11:08:40', '345999_20210709', '345999', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 13:34:37', '987659_20210709', '987659', 0.15, 'SMS', '2021-07-09')\n",
      "('2021-08-16 06:33:53', '987658_20210709', '987658', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 08:37:06', '987657_20210709', '987657', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 09:06:10', '987656_20210709', '987656', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 03:56:28', '987655_20210709', '987655', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 10:16:02', '987654_20210709', '987654', 0.75, 'SMS', '2021-07-09')\n",
      "('2021-08-16 05:37:03', '987653_20210709', '987653', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 02:17:26', '987652_20210709', '987652', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 09:42:33', '987651_20210709', '987651', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 10:57:45', '798321_20210709', '798321', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 11:10:23', '798322_20210709', '798322', 0.75, 'SMS', '2021-07-09')\n",
      "('2021-08-16 08:14:20', '798323_20210709', '798323', 0.15, 'SMS', '2021-07-09')\n",
      "('2021-08-16 02:04:00', '798324_20210709', '798324', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 05:36:15', '798325_20210709', '798325', 0.75, 'SMS', '2021-07-09')\n",
      "('2021-08-16 02:55:43', '798326_20210709', '798326', 0.15, 'SMS', '2021-07-09')\n",
      "('2021-08-16 03:43:20', '798327_20210709', '798327', 0.15, 'SMS', '2021-07-09')\n",
      "('2021-08-16 12:38:45', '798328_20210709', '798328', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 13:05:08', '798329_20210709', '798329', 0.3, 'SMS', '2021-07-09')\n",
      "('2021-08-16 04:45:30', '654321_20210709', '654321', 0.75, 'SMS', '2021-07-09')\n",
      "('2021-08-16 05:32:25', '543211_20201222', '543211', 1.5, 'SMS', '2020-12-22')\n",
      "('2021-08-16 11:21:08', '543212_20201222', '543212', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 11:54:33', '543213_20201222', '543213', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 09:47:11', '543214_20201222', '543214', 0.75, 'SMS', '2020-12-22')\n",
      "('2021-08-16 08:08:18', '543215_20201222', '543215', 0.75, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:37:23', '543217_20201222', '543217', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 09:19:31', '543218_20201222', '543218', 0.75, 'SMS', '2020-12-22')\n",
      "('2021-08-16 09:32:02', '543219_20201222', '543219', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 11:01:10', '543220_20201222', '543220', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 11:29:47', '543221_20201222', '543221', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:43:09', '543222_20201222', '543222', 0.75, 'SMS', '2020-12-22')\n",
      "('2021-08-16 09:20:50', '543223_20201222', '543223', 1.5, 'SMS', '2020-12-22')\n",
      "('2021-08-16 08:55:06', '543224_20201222', '543224', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 09:09:22', '543225_20201222', '543225', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:16:53', '543227_20201222', '543227', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 04:36:24', '543228_20201222', '543228', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 07:25:46', '543229_20201222', '543229', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 09:55:36', '876540_20201222', '876540', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:31:58', '345991_20201222', '345991', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 04:37:59', '876541_20201222', '876541', 0.3, 'USSD', '2020-12-22')\n",
      "('2021-08-16 11:00:19', '876542_20201222', '876542', 1.5, 'SMS', '2020-12-22')\n",
      "('2021-08-16 11:54:26', '876543_20201222', '876543', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:08:35', '876544_20201222', '876544', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 06:04:01', '876545_20201222', '876545', 1.5, 'SMS', '2020-12-22')\n",
      "('2021-08-16 08:20:07', '876546_20201222', '876546', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 09:27:55', '876547_20201222', '876547', 0.75, 'SMS', '2020-12-22')\n",
      "('2021-08-16 03:52:21', '876548_20201222', '876548', 0.15, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:51:45', '876549_20201222', '876549', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:14:25', '739280_20201222', '739280', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 02:14:01', '123456_20201222', '123456', 0.3, 'SMS', '2020-12-22')\n",
      "('2021-08-16 11:16:58', '739281_20201222', '739281', 0.3, 'SMS', '2020-12-22')\n",
      "              timestamp          row_key  sub_id  amount channel      act_dt\n",
      "0   2021-08-16 02:14:01  123456_20210810  123456    0.30     SMS  2021-08-10\n",
      "1   2021-08-16 02:54:43  123451_20210810  123451    0.15     SMS  2021-08-10\n",
      "2   2021-08-16 02:04:29  123452_20210810  123452    0.15     SMS  2021-08-10\n",
      "3   2021-08-16 02:39:05  123453_20210810  123453    0.15     SMS  2021-08-10\n",
      "4   2021-08-16 02:14:19  123454_20210810  123454    0.30     SMS  2021-08-10\n",
      "..                  ...              ...     ...     ...     ...         ...\n",
      "71  2021-08-16 03:52:21  876548_20201222  876548    0.15     SMS  2020-12-22\n",
      "72  2021-08-16 02:51:45  876549_20201222  876549    0.30     SMS  2020-12-22\n",
      "73  2021-08-16 02:14:25  739280_20201222  739280    0.30     SMS  2020-12-22\n",
      "74  2021-08-16 02:14:01  123456_20201222  123456    0.30     SMS  2020-12-22\n",
      "75  2021-08-16 11:16:58  739281_20201222  739281    0.30     SMS  2020-12-22\n",
      "\n",
      "[76 rows x 6 columns]\n",
      "Parquet file saved successfully from Pandas DataFrame!\n",
      "Error: UNIQUE constraint failed: non_common_subID.sub_id\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, date_format, to_timestamp\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "def main():\n",
    "    # Load the csv files\n",
    "    subscribers_file = r\"C:\\Users\\nikol\\Desktop\\Data_Engineer_Assessment\\subscribers.csv\"\n",
    "    transactions_file = r\"C:\\Users\\nikol\\Desktop\\Data_Engineer_Assessment\\data_transactions.csv\"\n",
    "\n",
    "    # Create a Spark Session to create the Spark App\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Transactions with Subscribers App\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Spark read the 2 datasets \n",
    "    subscribers_df = spark.read.csv(subscribers_file, header=False, inferSchema=True)\n",
    "    transactions_df = spark.read.csv(transactions_file, header=False, inferSchema=True)\n",
    "\n",
    "    # Clean the transactions data, subscribers data is cleaned\n",
    "    transactions_df = transactions_df.toDF(\"timestamp\", \"subscriber_id\", \"amount\", \"channel\")\n",
    "    subscribers_df = subscribers_df.toDF(\"subscriber_id\", \"activation_date\")\n",
    "\n",
    "    # Convert subscriber_id to integer and then back to string in order to load it in the db as text\n",
    "    # I want it to later compare it with the other dataset's subID that is string as well, so must have same type\n",
    "    transactions_df = transactions_df.withColumn(\"subscriber_id\", col(\"subscriber_id\").cast(\"int\"))\n",
    "    transactions_df = transactions_df.withColumn(\"subscriber_id\", col(\"subscriber_id\").cast(\"string\"))\n",
    "\n",
    "    # Filter rows/ Clean the data\n",
    "    filtered_df = transactions_df.filter(col(\"timestamp\").rlike(r\"^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\+\\d{2}$\"))  # Matches the timestamp\n",
    "    filtered_df = filtered_df.filter(col(\"amount\").rlike(r\"\\.\"))  # Cleans out non double values\n",
    "    filtered_df = filtered_df.filter(col(\"channel\").rlike(r\"^[a-zA-Z]+$\"))  # Keeps only letters (SMS, USSD)\n",
    "\n",
    "    # Keep only subscriber_id values that have exactly 6 digits and convert them to strings\n",
    "    filtered_df = (\n",
    "        filtered_df\n",
    "        .filter(col(\"subscriber_id\").rlike(r\"^\\d{6}$\"))  # Match only 6-digit values\n",
    "        .withColumn(\"subscriber_id\", col(\"subscriber_id\").cast(\"string\"))  # Ensure subscriber_id is a string\n",
    "    )\n",
    "\n",
    "    # Remove rows with null or empty values\n",
    "    filtered_df = filtered_df.na.drop()\n",
    "\n",
    "    # Convert column types\n",
    "    converted_df = (\n",
    "        filtered_df\n",
    "        .withColumn(\"timestamp\", to_timestamp(\"timestamp\"))  # Convert timestamp\n",
    "        .withColumn(\"amount\", col(\"amount\").cast(DecimalType(10, 4)))  # Convert amount to decimal\n",
    "    )\n",
    "\n",
    "    # Checking if this is the correct one..\n",
    "    # print(\"Cleansed Transactions DataFrame:\")\n",
    "    # converted_df.show(150)\n",
    "\n",
    "    # SQLite setup\n",
    "    sqlite_db_path = r\"C:\\Users\\nikol\\Desktop\\Data_Engineer_Assessment\\nikosdb.db\"\n",
    "    conn = sqlite3.connect(sqlite_db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create subscribers and table if it doesn't exist, in order to have it prepared\n",
    "        cursor.execute('''CREATE TABLE IF NOT EXISTS subscribers (\n",
    "            row_key TEXT PRIMARY KEY,\n",
    "            sub_id TEXT,\n",
    "            activation_date TEXT\n",
    "        )''')\n",
    "\n",
    "        # Create transactions table if it doesn't exist\n",
    "        cursor.execute('''CREATE TABLE IF NOT EXISTS transactions (\n",
    "            timestamp TEXT,\n",
    "            sub_id TEXT,\n",
    "            amount DECIMAL(10, 4),\n",
    "            channel TEXT\n",
    "        )''')\n",
    "\n",
    "        # Prepare subscribers df to load it into the db table\n",
    "        prepared_subscribers_df = (\n",
    "            subscribers_df\n",
    "            .withColumn(\"row_key\", concat_ws(\"_\", col(\"subscriber_id\"), date_format(\"activation_date\", \"yyyyMMdd\")) )\n",
    "            .select(\n",
    "                col(\"row_key\"),\n",
    "                col(\"subscriber_id\").alias(\"sub_id\"),\n",
    "                col(\"activation_date\").alias(\"act_dt\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Insert the prepared df into SQLite subscribers table\n",
    "        for row in prepared_subscribers_df.collect():\n",
    "            cursor.execute('''INSERT OR REPLACE INTO subscribers (row_key, sub_id, activation_date)\n",
    "                              VALUES (?, ?, ?)''', (row['row_key'], row['sub_id'], row['act_dt']))\n",
    "\n",
    "        # Prepare transactions df (cleaned data)\n",
    "        prepared_transactions_df = converted_df.select(\n",
    "            col(\"timestamp\"),\n",
    "            col(\"subscriber_id\").alias(\"sub_id\"),\n",
    "            col(\"amount\"),\n",
    "            col(\"channel\")\n",
    "        )\n",
    "\n",
    "        # Insert cleaned transaction into SQLite transactions table and then insert it into the db table\n",
    "        transactions_data = [\n",
    "            (row['timestamp'].strftime('%Y-%m-%d %H:%M:%S'),  # Convert datetime to string\n",
    "             row['sub_id'],\n",
    "             float(row['amount']),\n",
    "             row['channel']) \n",
    "            for row in prepared_transactions_df.collect()\n",
    "        ]\n",
    "        \n",
    "        # print(transactions_data)\n",
    "\n",
    "        cursor.executemany('''INSERT INTO transactions (timestamp, sub_id, amount, channel)\n",
    "                               VALUES (?, ?, ?, ?)''', transactions_data)\n",
    "        \n",
    "        # Perform INNER JOIN between 2 tables to get the common subscriber IDs\n",
    "        # Also, selecting the values needed for the parquet file later\n",
    "        cursor.execute('''\n",
    "            SELECT\n",
    "                transactions.timestamp,\n",
    "                subscribers.row_key,\n",
    "                subscribers.sub_id AS sub_id,  \n",
    "                transactions.amount,\n",
    "                transactions.channel,\n",
    "                subscribers.activation_date AS act_dt\n",
    "            FROM\n",
    "                subscribers \n",
    "            INNER JOIN\n",
    "                transactions \n",
    "            ON\n",
    "                subscribers.sub_id = transactions.sub_id  \n",
    "        ''')\n",
    "\n",
    "        # Fetch all the results of the query\n",
    "        result = cursor.fetchall()\n",
    "\n",
    "        import pandas as pd\n",
    "        # Print the result of the JOIN\n",
    "        print(\"INNER JOIN Result:\")\n",
    "        for row in result:\n",
    "            print(row)\n",
    "\n",
    "        # Convert the query result to create a pandas dataframe to then convert the pd df into parquet file\n",
    "        result_df = pd.DataFrame(result, columns=[\"timestamp\", \"row_key\", \"sub_id\", \"amount\", \"channel\", \"act_dt\"])\n",
    "        print(result_df)\n",
    "\n",
    "        result_df.to_parquet(\n",
    "            r\"C:\\Users\\nikol\\Desktop\\Data_Engineer_Assessment\\joined_results.parquet\",\n",
    "            engine='pyarrow',  \n",
    "            index=False\n",
    "            )\n",
    "        print(\"Parquet file saved successfully from Pandas DataFrame!\")\n",
    "\n",
    "\n",
    "        #########   MAIN OBJECTIVE ENDS HERE   #########\n",
    "\n",
    "        # I will create a db table in order to have the non common subIDs stored for later retrieval\n",
    "\n",
    "        # Create non_common_subI' table\n",
    "        cursor.execute('''CREATE TABLE IF NOT EXISTS non_common_subID (\n",
    "            sub_id TEXT PRIMARY KEY\n",
    "        )''')\n",
    "        \n",
    "        # Perform FULL OUTER JOIN emulation to find non-common sub_id values\n",
    "        # FULL OUTER JOIN == 2 LEFT JOINS (or left and right) \n",
    "        cursor.execute('''\n",
    "            INSERT INTO non_common_subID (sub_id)\n",
    "            SELECT sub_id FROM (\n",
    "                SELECT subscribers.sub_id \n",
    "                FROM subscribers\n",
    "                LEFT JOIN transactions\n",
    "                ON subscribers.sub_id = transactions.sub_id\n",
    "                WHERE transactions.sub_id IS NULL\n",
    "                UNION\n",
    "                SELECT transactions.sub_id \n",
    "                FROM transactions\n",
    "                LEFT JOIN subscribers\n",
    "                ON transactions.sub_id = subscribers.sub_id\n",
    "                WHERE subscribers.sub_id IS NULL\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Fetch and print the non-common sub_id values\n",
    "        cursor.execute('SELECT * FROM non_common_subID')\n",
    "        non_common_sub_ids = cursor.fetchall()\n",
    "        \n",
    "        print(\"Non-Common sub_ids:\")\n",
    "        for row in non_common_sub_ids:\n",
    "            print(row)\n",
    "\n",
    "        conn.commit() \n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        conn.close()  # Close the connection\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
